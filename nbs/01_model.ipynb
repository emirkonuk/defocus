{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "> Here be the code for the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.optim import Adam\n",
    "from adamp import AdamP\n",
    "from torch.nn.parallel import DistributedDataParallel, DataParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.G = None\n",
    "        self.D = None\n",
    "        self.P = None\n",
    "        self.G_optimizer = None\n",
    "        self.D_optimizer = None\n",
    "        self.reconstruction_loss = None\n",
    "        self.adversarial_loss = None\n",
    "        self.perceptual_loss = None\n",
    "                    \n",
    "    def use_perceptual(self, after_activation=False):\n",
    "        # this is from DeblurGANv2\n",
    "        # TODO: ESRGAN's perceptual loss version\n",
    "        conv_3_3_layer = 14\n",
    "        cnn = torchvision.models.vgg19(pretrained=True).features\n",
    "        perceptual = nn.Sequential()\n",
    "        perceptual = perceptual.eval()\n",
    "        for i, layer in enumerate(list(cnn)):\n",
    "            perceptual.add_module(str(i), layer)\n",
    "            if i == conv_3_3_layer:\n",
    "                break\n",
    "        self.P = perceptual\n",
    "        \n",
    "    def set_resconstruction_loss(self, loss_functions=[nn.MSELoss], weights=[1.0]):\n",
    "        def weighted_loss(input_, target):\n",
    "            total = 0\n",
    "            for (func, weight) in zip(loss_functions, weights):\n",
    "                total += func(input_, target)*weight\n",
    "            return total\n",
    "        self.reconstruction_loss = weighted_loss\n",
    "        \n",
    "    def set_adversarial_loss(self, loss_functions=[nn.BCEWithLogitsLoss], weights=[1.0]):\n",
    "        def weighted_loss(input_, target):\n",
    "            total = 0\n",
    "            for (func, weight) in zip(loss_functions, weights):\n",
    "                total += func(input_, target)*weight\n",
    "            return total\n",
    "        self.adversarial_loss = weighted_loss\n",
    "        \n",
    "    def set_perceptual_loss(self, loss_functions=[nn.L1Loss], weights=[1.0]):\n",
    "        def weighted_loss(input_, target):\n",
    "            total = 0\n",
    "            for (func, weight) in zip(loss_functions, weights):\n",
    "                total += func(input_, target)*weight\n",
    "            return total\n",
    "        self.perceptual_loss = weighted_loss\n",
    "        \n",
    "    def set_G_optimizer(self, optimizer='AdamP', lr=1e-4, betas=(0.9, 0.999), weight_decay=0, nesterov=False):\n",
    "        # note that lucidrains uses betas=(0.5, 0.9) for stylegan\n",
    "        # https://github.com/lucidrains/stylegan2-pytorch/blob/master/stylegan2_pytorch/stylegan2_pytorch.py#L565        \n",
    "        \n",
    "        if optimizer=='Adam':\n",
    "            self.G_optimizer = Adam(self.G.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n",
    "        elif optimizer=='AdamP':\n",
    "            self.G_optimizer = AdamP(self.G.parameters(), lr=lr, betas=betas, weight_decay=weight_decay, nesterov=nesterov)\n",
    "        else:\n",
    "            #TODO: other optimizers, maybe from the torch_optimizers package\n",
    "            raise NotImplementedError('nope')\n",
    "                \n",
    "    def set_D_optimizer(self, optimizer='AdamP', lr=1e-4, betas=(0.9, 0.999), weight_decay=0, nesterov=False):\n",
    "        # note that lucidrains uses betas=(0.5, 0.9) for stylegan\n",
    "        # https://github.com/lucidrains/stylegan2-pytorch/blob/master/stylegan2_pytorch/stylegan2_pytorch.py#L565        \n",
    "        \n",
    "        if optimizer=='Adam':\n",
    "            self.D_optimizer = Adam(self.D.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n",
    "        elif optimizer=='AdamP':\n",
    "            self.D_optimizer = AdamP(self.D.parameters(), lr=lr, betas=betas, weight_decay=weight_decay, nesterov=nesterov)\n",
    "        else:\n",
    "            #TODO: other optimizers, maybe from the torch_optimizers package\n",
    "            raise NotImplementedError('nope')\n",
    "            \n",
    "    def save(self, model_path):\n",
    "        if isinstance(self.G, DataParallel) or isinstance(self.G, DistributedDataParallel):\n",
    "            G_state_dict = self.G.module.state_dict()\n",
    "            D_state_dict = self.D.module.state_dict()\n",
    "        else:\n",
    "            G_state_dict = self.G.state_dict()\n",
    "            D_state_dict = self.D.state_dict()\n",
    "        torch.save({'G':G_state_dict,\n",
    "                    'D':D_state_dict,\n",
    "                    'optimizer_G': self.G_optimizer.state_dict(),\n",
    "                    'optimizer_D': self.D_optimizer.state_dict()}, \n",
    "                   model_path)\n",
    "        \n",
    "    def load(self, model_path, isStrict=False, map_location='cpu'):\n",
    "        # don't care about distributed here\n",
    "        # trainer should take care of it after loading the parameters\n",
    "        checkpoint = torch.load(model_path, map_location=map_location)\n",
    "        self.G.load_state_dict(checkpoint['G'], strict=isStrict)\n",
    "        self.D.load_state_dict(checkpoint['D'], strict=isStrict)\n",
    "        self.G_optimizer.load_state_dict(checkpoint['optimizer_G'])\n",
    "        self.D_optimizer.load_state_dict(checkpoint['optimizer_D'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01_model.ipynb.\n",
      "Converted 02_architecture_common.ipynb.\n",
      "Converted 03_architecture_MSResNet.ipynb.\n",
      "Converted 04_dataset_common.ipynb.\n",
      "Converted 05_dataset_MSResNet.ipynb.\n",
      "Converted 06_trainer_MSResNet.ipynb.\n",
      "Converted 99_diffaugment.ipynb.\n",
      "Converted Tutorial.ipynb.\n",
      "Converted lightning_trial.ipynb.\n",
      "Converted trials.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fp16] *",
   "language": "python",
   "name": "conda-env-fp16-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

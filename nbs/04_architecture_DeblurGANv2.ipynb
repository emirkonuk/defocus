{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp architecture.DeblurGANv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import functools\n",
    "import torch.nn.functional as F\n",
    "\n",
    "'''\n",
    "TODO: as soon as (ever?) torchvision has this, switch to torchvision\n",
    "'''\n",
    "from pretrainedmodels import inceptionresnetv2\n",
    "\n",
    "'''\n",
    "Copied from DeblurGANv2 repo.\n",
    "https://github.com/VITA-Group/DeblurGANv2\n",
    "TODO: simplify the code. seriously.\n",
    "'''\n",
    "instance_norm = functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=True)\n",
    "# Defines the PatchGAN discriminator with the specified arguments.\n",
    "# with n_layers=5 it is the full-gan\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_nc=3, ndf=64, n_layers=5, norm_layer=instance_norm, use_sigmoid=False, use_parallel=True):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.use_parallel = use_parallel\n",
    "        if type(norm_layer) == functools.partial:\n",
    "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
    "        else:\n",
    "            use_bias = norm_layer == nn.InstanceNorm2d\n",
    "\n",
    "        kw = 4\n",
    "        padw = int(np.ceil((kw-1)/2))\n",
    "        sequence = [\n",
    "            nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        ]\n",
    "\n",
    "        nf_mult = 1\n",
    "        for n in range(1, n_layers):\n",
    "            nf_mult_prev = nf_mult\n",
    "            nf_mult = min(2**n, 8)\n",
    "            sequence += [\n",
    "                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n",
    "                          kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n",
    "                norm_layer(ndf * nf_mult),\n",
    "                nn.LeakyReLU(0.2, True)\n",
    "            ]\n",
    "\n",
    "        nf_mult_prev = nf_mult\n",
    "        nf_mult = min(2**n_layers, 8)\n",
    "        sequence += [\n",
    "            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n",
    "                      kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n",
    "            norm_layer(ndf * nf_mult),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        ]\n",
    "\n",
    "        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]\n",
    "\n",
    "        if use_sigmoid:\n",
    "            sequence += [nn.Sigmoid()]\n",
    "\n",
    "        self.model = nn.Sequential(*sequence)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.model(input)\n",
    "    \n",
    "    \n",
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, norm_layer=instance_norm, output_ch=3, num_filters=128, num_filters_fpn=256):\n",
    "        super().__init__()\n",
    "\n",
    "        # Feature Pyramid Network (FPN) with four feature maps of resolutions\n",
    "        # 1/4, 1/8, 1/16, 1/32 and `num_filters` filters for all feature maps.\n",
    "        self.fpn = FPN(num_filters=num_filters_fpn, norm_layer=norm_layer)\n",
    "\n",
    "        # The segmentation heads on top of the FPN\n",
    "\n",
    "        self.head1 = FPNHead(num_filters_fpn, num_filters, num_filters)\n",
    "        self.head2 = FPNHead(num_filters_fpn, num_filters, num_filters)\n",
    "        self.head3 = FPNHead(num_filters_fpn, num_filters, num_filters)\n",
    "        self.head4 = FPNHead(num_filters_fpn, num_filters, num_filters)\n",
    "\n",
    "        self.smooth = nn.Sequential(\n",
    "            nn.Conv2d(4 * num_filters, num_filters, kernel_size=3, padding=1),\n",
    "            norm_layer(num_filters),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.smooth2 = nn.Sequential(\n",
    "            nn.Conv2d(num_filters, num_filters // 2, kernel_size=3, padding=1),\n",
    "            norm_layer(num_filters // 2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.final = nn.Conv2d(num_filters // 2, output_ch, kernel_size=3, padding=1)\n",
    "\n",
    "\n",
    "    def unfreeze(self):\n",
    "        self.fpn.unfreeze()\n",
    "\n",
    "    def forward(self, x):\n",
    "        map0, map1, map2, map3, map4 = self.fpn(x)\n",
    "\n",
    "        map4 = nn.functional.interpolate(self.head4(map4), scale_factor=8, mode=\"nearest\")\n",
    "        map3 = nn.functional.interpolate(self.head3(map3), scale_factor=4, mode=\"nearest\")\n",
    "        map2 = nn.functional.interpolate(self.head2(map2), scale_factor=2, mode=\"nearest\")\n",
    "        map1 = nn.functional.interpolate(self.head1(map1), scale_factor=1, mode=\"nearest\")\n",
    "\n",
    "        smoothed = self.smooth(torch.cat([map4, map3, map2, map1], dim=1))\n",
    "        smoothed = nn.functional.interpolate(smoothed, scale_factor=2, mode=\"nearest\")\n",
    "        smoothed = self.smooth2(smoothed + map0)\n",
    "        smoothed = nn.functional.interpolate(smoothed, scale_factor=2, mode=\"nearest\")\n",
    "\n",
    "        final = self.final(smoothed)\n",
    "        res = torch.tanh(final) + x\n",
    "\n",
    "        return torch.clamp(res, min = -1,max = 1)\n",
    "    \n",
    "    \n",
    "class FPNHead(nn.Module):\n",
    "    def __init__(self, num_in, num_mid, num_out):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block0 = nn.Conv2d(num_in, num_mid, kernel_size=3, padding=1, bias=False)\n",
    "        self.block1 = nn.Conv2d(num_mid, num_out, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.block0(x), inplace=True)\n",
    "        x = nn.functional.relu(self.block1(x), inplace=True)\n",
    "        return x    \n",
    "    \n",
    "    \n",
    "class FPN(nn.Module):\n",
    "\n",
    "    def __init__(self, norm_layer=instance_norm, num_filters=256):\n",
    "        \"\"\"Creates an `FPN` instance for feature extraction.\n",
    "        Args:\n",
    "          num_filters: the number of filters in each output pyramid level\n",
    "          pretrained: use ImageNet pre-trained backbone feature extractor\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.inception = inceptionresnetv2(num_classes=1000, pretrained='imagenet')\n",
    "\n",
    "        self.enc0 = self.inception.conv2d_1a\n",
    "        self.enc1 = nn.Sequential(\n",
    "            self.inception.conv2d_2a,\n",
    "            self.inception.conv2d_2b,\n",
    "            self.inception.maxpool_3a,\n",
    "        ) # 64\n",
    "        self.enc2 = nn.Sequential(\n",
    "            self.inception.conv2d_3b,\n",
    "            self.inception.conv2d_4a,\n",
    "            self.inception.maxpool_5a,\n",
    "        )  # 192\n",
    "        self.enc3 = nn.Sequential(\n",
    "            self.inception.mixed_5b,\n",
    "            self.inception.repeat,\n",
    "            self.inception.mixed_6a,\n",
    "        )   # 1088\n",
    "        self.enc4 = nn.Sequential(\n",
    "            self.inception.repeat_1,\n",
    "            self.inception.mixed_7a,\n",
    "        ) #2080\n",
    "        self.td1 = nn.Sequential(nn.Conv2d(num_filters, num_filters, kernel_size=3, padding=1),\n",
    "                                 norm_layer(num_filters),\n",
    "                                 nn.ReLU(inplace=True))\n",
    "        self.td2 = nn.Sequential(nn.Conv2d(num_filters, num_filters, kernel_size=3, padding=1),\n",
    "                                 norm_layer(num_filters),\n",
    "                                 nn.ReLU(inplace=True))\n",
    "        self.td3 = nn.Sequential(nn.Conv2d(num_filters, num_filters, kernel_size=3, padding=1),\n",
    "                                 norm_layer(num_filters),\n",
    "                                 nn.ReLU(inplace=True))\n",
    "        self.pad = nn.ReflectionPad2d(1)\n",
    "        self.lateral4 = nn.Conv2d(2080, num_filters, kernel_size=1, bias=False)\n",
    "        self.lateral3 = nn.Conv2d(1088, num_filters, kernel_size=1, bias=False)\n",
    "        self.lateral2 = nn.Conv2d(192, num_filters, kernel_size=1, bias=False)\n",
    "        self.lateral1 = nn.Conv2d(64, num_filters, kernel_size=1, bias=False)\n",
    "        self.lateral0 = nn.Conv2d(32, num_filters // 2, kernel_size=1, bias=False)\n",
    "\n",
    "        for param in self.inception.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def unfreeze(self):\n",
    "        for param in self.inception.parameters():\n",
    "            param.requires_grad = True\n",
    "        print(\"Unfreeze successful.\")\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Bottom-up pathway, from ResNet\n",
    "        enc0 = self.enc0(x)\n",
    "\n",
    "        enc1 = self.enc1(enc0) # 256\n",
    "\n",
    "        enc2 = self.enc2(enc1) # 512\n",
    "\n",
    "        enc3 = self.enc3(enc2) # 1024\n",
    "\n",
    "        enc4 = self.enc4(enc3) # 2048\n",
    "\n",
    "        # Lateral connections\n",
    "\n",
    "        lateral4 = self.pad(self.lateral4(enc4))\n",
    "        lateral3 = self.pad(self.lateral3(enc3))\n",
    "        lateral2 = self.lateral2(enc2)\n",
    "        lateral1 = self.pad(self.lateral1(enc1))\n",
    "        lateral0 = self.lateral0(enc0)\n",
    "\n",
    "        # Top-down pathway\n",
    "        pad = (1, 2, 1, 2)  # pad last dim by 1 on each side\n",
    "        pad1 = (0, 1, 0, 1)\n",
    "        map4 = lateral4\n",
    "        map3 = self.td1(lateral3 + nn.functional.interpolate(map4, scale_factor=2, mode=\"nearest\"))\n",
    "        map2 = self.td2(F.pad(lateral2, pad, \"reflect\") + nn.functional.interpolate(map3, scale_factor=2, mode=\"nearest\"))\n",
    "        map1 = self.td3(lateral1 + nn.functional.interpolate(map2, scale_factor=2, mode=\"nearest\"))\n",
    "        return F.pad(lateral0, pad1, \"reflect\"), map1, map2, map3, map4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01_model.ipynb.\n",
      "Converted 02_architecture_common.ipynb.\n",
      "Converted 03_architecture_MSResNet.ipynb.\n",
      "Converted 04_dataset_common.ipynb.\n",
      "Converted 05_dataset_MSResNet.ipynb.\n",
      "Converted 06_trainer_MSResNet.ipynb.\n",
      "Converted 07_metrics.ipynb.\n",
      "Converted 08_architecture_DeblurGANv2.ipynb.\n",
      "Converted 09_dataset_DeblurGANv2.ipynb.\n",
      "Converted 10_losses.ipynb.\n",
      "Converted 99_basemodel.ipynb.\n",
      "Converted 99_dataset_DeblurGANv2_clean.ipynb.\n",
      "Converted 99_diffaugment.ipynb.\n",
      "Converted 99_model_DeblurGANv2_clean.ipynb.\n",
      "Converted 99_model_MSResNet.ipynb.\n",
      "Converted DeblurGANv2_lightning-from-vanilla-Copy1.ipynb.\n",
      "Converted DeblurGANv2_lightning-from-vanilla.ipynb.\n",
      "Converted DeblurGANv2_vanilla.ipynb.\n",
      "Converted Tutorial_without_lightning.ipynb.\n",
      "Converted fuckit.ipynb.\n",
      "Converted hmmm.ipynb.\n",
      "Converted model_without_lightning.ipynb.\n",
      "Converted trials.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_disc = Discriminator(n_layers=3,\n",
    "                           norm_layer=functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=True),\n",
    "                           use_sigmoid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(norm_layer=functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(1,3,720,1280)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, c, h, w = x.shape\n",
    "block_size= 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " min_height = (h // block_size + 1) * block_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_width = (w // block_size + 1) * block_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1312, 736)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_width, min_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_w = (min_width - w)//2\n",
    "pad_h = (min_height - h)//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " pad_params = {'mode': 'constant',\n",
    "                      'value': 0,\n",
    "                      'pad': (pad_w, pad_w, pad_h, pad_h)\n",
    "                      }\n",
    "x = F.pad(x, **pad_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 736, 1312])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0153,  0.1499, -0.2312,  ..., -0.3292, -0.1599, -0.0901],\n",
       "          [-0.1745, -0.0168,  0.2872,  ..., -0.3346, -0.1249,  0.0216],\n",
       "          [-0.5284, -0.3765, -0.1533,  ..., -0.1632, -0.0262,  0.0801],\n",
       "          ...,\n",
       "          [ 0.0500, -0.0424,  0.0689,  ..., -0.1204, -0.0469,  0.0082],\n",
       "          [ 0.0857,  0.0094,  0.0410,  ..., -0.2651, -0.2845, -0.1377],\n",
       "          [ 0.0975, -0.0058, -0.2135,  ..., -0.3063, -0.3515, -0.0448]],\n",
       "\n",
       "         [[-0.3389,  0.0734, -0.0703,  ..., -0.0618, -0.0698,  0.1708],\n",
       "          [ 0.1390,  0.3178,  0.3994,  ...,  0.1895,  0.0259,  0.2192],\n",
       "          [-0.2641, -0.0761,  0.3369,  ...,  0.4438,  0.0495,  0.1800],\n",
       "          ...,\n",
       "          [-0.0663,  0.3616,  0.3557,  ..., -0.4078, -0.3815, -0.0998],\n",
       "          [-0.2381,  0.0448,  0.0123,  ..., -0.5223, -0.4879, -0.3241],\n",
       "          [-0.1543, -0.1696, -0.2505,  ..., -0.2275, -0.2418, -0.0958]],\n",
       "\n",
       "         [[ 0.2883,  0.0787, -0.2844,  ..., -0.2128, -0.1868,  0.0233],\n",
       "          [ 0.1258, -0.5242, -0.6848,  ..., -0.0322,  0.1197,  0.1487],\n",
       "          [-0.0699, -0.7418, -0.7939,  ..., -0.2081,  0.0129,  0.2449],\n",
       "          ...,\n",
       "          [-0.0426,  0.0836,  0.1486,  ...,  0.0368,  0.2450,  0.4905],\n",
       "          [-0.1400, -0.0782, -0.2177,  ..., -0.1603,  0.1000,  0.2537],\n",
       "          [-0.2210, -0.1120, -0.1834,  ...,  0.0309,  0.1298,  0.0305]]]],\n",
       "       grad_fn=<ClampBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc0 = generator.fpn.enc0(x)\n",
    "enc1 = generator.fpn.enc1(enc0)\n",
    "enc2 = generator.fpn.enc2(enc1)\n",
    "enc3 = generator.fpn.enc3(enc2)\n",
    "enc4 = generator.fpn.enc4(enc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lateral4 = generator.fpn.pad(generator.fpn.lateral4(enc4))\n",
    "lateral3 = generator.fpn.pad(generator.fpn.lateral3(enc3))\n",
    "lateral2 = generator.fpn.lateral2(enc2)\n",
    "lateral1 = generator.fpn.pad(generator.fpn.lateral1(enc1))\n",
    "lateral0 = generator.fpn.lateral0(enc0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (45) must match the size of tensor b (46) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-439a2d1ca6ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpad1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmap4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlateral4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmap3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfpn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtd1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlateral3\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"nearest\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (45) must match the size of tensor b (46) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "pad = (1, 2, 1, 2)  # pad last dim by 1 on each side\n",
    "pad1 = (0, 1, 0, 1)\n",
    "map4 = lateral4\n",
    "map3 = generator.fpn.td1(lateral3 + nn.functional.interpolate(map4, scale_factor=2, mode=\"nearest\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 46, 80])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.functional.upsample(map4, scale_factor=2, mode=\"nearest\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc0 torch.Size([1, 32, 359, 639])\n",
      "enc1 torch.Size([1, 64, 178, 318])\n",
      "enc2 torch.Size([1, 192, 87, 157])\n",
      "enc2 torch.Size([1, 192, 87, 157])\n",
      "enc3 torch.Size([1, 1088, 43, 78])\n",
      "enc4 torch.Size([1, 2080, 21, 38])\n",
      "lateral4 torch.Size([1, 256, 23, 40])\n",
      "lateral3 torch.Size([1, 256, 45, 80])\n",
      "lateral2 torch.Size([1, 256, 87, 157])\n",
      "lateral1 torch.Size([1, 256, 180, 320])\n",
      "lateral0 torch.Size([1, 128, 359, 639])\n",
      "map4 torch.Size([1, 256, 23, 40])\n",
      "map3 torch.Size([1, 256, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "print('enc0', enc0.shape)\n",
    "print('enc1', enc1.shape)\n",
    "print('enc2', enc2.shape)\n",
    "print('enc2', enc2.shape)\n",
    "print('enc3', enc3.shape)\n",
    "print('enc4', enc4.shape)\n",
    "print('lateral4', lateral4.shape)\n",
    "print('lateral3', lateral3.shape)\n",
    "print('lateral2', lateral2.shape)\n",
    "print('lateral1', lateral1.shape)\n",
    "print('lateral0', lateral0.shape)\n",
    "print('map4', map4.shape)\n",
    "print('map3', map3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map2 = generator.fpn.td2(F.pad(lateral2, pad, \"reflect\") + nn.functional.interpolate(map3, scale_factor=2, mode=\"nearest\"))\n",
    "map1 = generator.fpn.td3(lateral1 + nn.functional.interpolate(map2, scale_factor=2, mode=\"nearest\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map2 torch.Size([1, 256, 32, 32])\n",
      "map1 torch.Size([1, 256, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "print('map2', map2.shape)\n",
    "print('map1', map1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp architecture.MSResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from defocus.architecture.common import *\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    '''\n",
    "    MSResNet Generator\n",
    "    '''\n",
    "    def __init__(self, n_scales = 3, n_feats=64, kernel_size=5, n_resblocks=19):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        # Seungjun does not scale the images down to [-1,1] but rather uses [-127.5, 127.5]\n",
    "        # this self.mean variable will be used for that. wonder why he calls it the \"mean\"?\n",
    "        self.mean = 255.0 / 2\n",
    "        self.n_scales = n_scales \n",
    "        \n",
    "        coarsest_level = ResNet(in_channels=3, \n",
    "                                out_channels=3,\n",
    "                                n_feats=n_feats,\n",
    "                                kernel_size=kernel_size,\n",
    "                                n_resblocks=n_resblocks)\n",
    "        finer_levels = [ResNet(in_channels=6,\n",
    "                               out_channels=3, \n",
    "                               n_feats=n_feats, \n",
    "                               kernel_size=kernel_size, \n",
    "                               n_resblocks=n_resblocks) for _ in range(n_scales-1)]\n",
    "               \n",
    "        self.scale_networks = nn.ModuleList([]) \n",
    "        self.scale_networks.append(coarsest_level)\n",
    "        self.scale_networks.extend(finer_levels)    \n",
    "        \n",
    "        # note that the original implementation always uses 5x5 kernels (default) for upsampling\n",
    "        self.upconv_blocks = nn.ModuleList([UpConv2D() for _ in range(n_scales-1)])\n",
    "        \n",
    "    def forward(self, input_pyramid):\n",
    "        \n",
    "        output_pyramid = [None]*self.n_scales\n",
    "        for scale in range(self.n_scales): \n",
    "            if scale == 0:\n",
    "                input_ = input_pyramid[scale] - self.mean\n",
    "            else:\n",
    "                upconvolved_from_previous = self.upconv_blocks[scale-1](output_pyramid[scale-1])\n",
    "                input_ = torch.cat((input_pyramid[scale] - self.mean, upconvolved_from_previous) ,1)\n",
    "                \n",
    "            output_pyramid[scale] = self.scale_networks[scale](input_)\n",
    "        \n",
    "        return output_pyramid       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch.nn as nn\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    '''\n",
    "    MSResNet Discriminator\n",
    "    '''\n",
    "    def __init__(self, n_feats=64, kernel_size=5):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # self.args = args\n",
    "#         n_feats = args.n_feats\n",
    "#         kernel_size = args.kernel_size\n",
    "\n",
    "        def conv(kernel_size, in_channel, n_feats, stride, pad=None):\n",
    "            if pad is None:\n",
    "                pad = (kernel_size-1)//2\n",
    "\n",
    "            return nn.Conv2d(in_channel, n_feats, kernel_size, stride=stride, padding=pad, bias=False)\n",
    "\n",
    "        self.conv_layers = nn.ModuleList([\n",
    "            conv(kernel_size, 3,         n_feats//2, 1),    # 256\n",
    "            conv(kernel_size, n_feats//2, n_feats//2, 2),   # 128\n",
    "            conv(kernel_size, n_feats//2, n_feats,   1),\n",
    "            conv(kernel_size, n_feats,   n_feats,   2),     # 64\n",
    "            conv(kernel_size, n_feats,   n_feats*2, 1),\n",
    "            conv(kernel_size, n_feats*2, n_feats*2, 4),     # 16\n",
    "            conv(kernel_size, n_feats*2, n_feats*4, 1),\n",
    "            conv(kernel_size, n_feats*4, n_feats*4, 4),     # 4\n",
    "            conv(kernel_size, n_feats*4, n_feats*8, 1),\n",
    "            conv(4,           n_feats*8, n_feats*8, 4, 0),  # 1\n",
    "        ])\n",
    "\n",
    "        self.act = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
    "        self.dense = nn.Conv2d(n_feats*8, 1, 1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        for layer in self.conv_layers:\n",
    "            x = self.act(layer(x))\n",
    "\n",
    "        x = self.dense(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01_model.ipynb.\n",
      "Converted 02_architecture_common.ipynb.\n",
      "Converted 03_architecture_MSResNet.ipynb.\n",
      "Converted 04_dataset_common.ipynb.\n",
      "Converted 05_dataset_MSResNet.ipynb.\n",
      "Converted 06_trainer.ipynb.\n",
      "Converted 99_diffaugment.ipynb.\n",
      "Converted trials.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:defocus]",
   "language": "python",
   "name": "conda-env-defocus-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

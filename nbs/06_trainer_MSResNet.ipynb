{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp trainers.MSResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import torch\n",
    "from torch.utils.data import SequentialSampler, RandomSampler, DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel, DataParallel\n",
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Trainer():\n",
    "    def __init__(self, \n",
    "                 model, \n",
    "                 train_dataset, validation_dataset, \n",
    "                 batch_size, \n",
    "                 num_workers, \n",
    "                 world_size=1,\n",
    "                 device_id=0 ):\n",
    "        \n",
    "        self.model = model\n",
    "        self.device_id = device_id\n",
    "        self.distributed = True if world_size>1 else False\n",
    "        \n",
    "        '''\n",
    "        Parallelization stuff\n",
    "        '''\n",
    "        if world_size==1:\n",
    "            train_sampler = RandomSampler(train_dataset, replacement=False)\n",
    "            validation_sampler = SequentialSampler(validation_dataset)\n",
    "            if self.device_id!='cpu':\n",
    "                self.model.cuda(self.device_id)         \n",
    "        else:\n",
    "            # if distributed, use the distributed sampler\n",
    "            train_sampler = DistributedSampler(train_dataset, \n",
    "                                               shuffle=True, \n",
    "                                               num_replicas=world_size, \n",
    "                                               rank=self.device_id,\n",
    "                                              )\n",
    "            validation_sampler = DistributedSampler(validation_dataset, \n",
    "                                                    shuffle=False, \n",
    "                                                    num_replicas=world_size, \n",
    "                                                    rank=self.device_id,\n",
    "                                                   )\n",
    "            # also, wrap the networks with DistributedDataParallel\n",
    "            if self.model.G is not None:\n",
    "                self.model.G.cuda(self.device_id)\n",
    "                self.model.G = DistributedDataParallel(self.model.G, device_ids=[self.device_id], output_device=self.device_id)\n",
    "            if self.model.D is not None:\n",
    "                self.model.D.cuda(self.device_id)\n",
    "                self.model.D = DistributedDataParallel(self.model.D, device_ids=[self.device_id], output_device=self.device_id)\n",
    "            if self.model.P is not None:\n",
    "                self.model.P.cuda(self.device_id)\n",
    "                self.model.P = DistributedDataParallel(self.model.P, device_ids=[self.device_id], output_device=self.device_id)\n",
    "            \n",
    "        self.train_loader = DataLoader(dataset=train_dataset,\n",
    "                                       batch_size=batch_size,\n",
    "                                       shuffle=False,\n",
    "                                       sampler=train_sampler,\n",
    "                                       num_workers=num_workers,\n",
    "                                       pin_memory=True,\n",
    "                                       drop_last=True,\n",
    "                                      )\n",
    "        self.val_loader = DataLoader(dataset=validation_dataset,\n",
    "                                     batch_size=batch_size,\n",
    "                                     shuffle=False,\n",
    "                                     sampler=validation_sampler,\n",
    "                                     num_workers=num_workers,\n",
    "                                     pin_memory=True,\n",
    "                                     drop_last=False,\n",
    "                                    )\n",
    "        self.count = 0\n",
    "        self.synchronized = False\n",
    "        \n",
    "    def all_reduce(self, epoch=None):\n",
    "        # synchronize loss for distributed GPU processes\n",
    "\n",
    "        if epoch is None:\n",
    "            epoch = self.epoch\n",
    "\n",
    "        def _reduce_value(value, ReduceOp=dist.ReduceOp.SUM):\n",
    "            value_tensor = torch.Tensor([value]).to(self.device_id, non_blocking=True)\n",
    "            dist.all_reduce(value_tensor, ReduceOp, async_op=False)\n",
    "            value = value_tensor.item()\n",
    "            del value_tensor\n",
    "\n",
    "            return value\n",
    "\n",
    "        dist.barrier()\n",
    "        if self.count > 0: \n",
    "            self.count = _reduce_value(self.count, dist.ReduceOp.SUM)\n",
    "\n",
    "            \n",
    "            '''\n",
    "            # this part is TODO\n",
    "            for loss_type in self.loss_stat[self.mode]:\n",
    "                self.loss_stat[self.mode][loss_type][epoch] = _reduce_value(\n",
    "                    self.loss_stat[self.mode][loss_type][epoch],\n",
    "                    dist.ReduceOp.SUM)\n",
    "                '''\n",
    "        self.synchronized = True\n",
    "\n",
    "        return\n",
    "   \n",
    "    def train(self, epoch):\n",
    "        self.epoch = epoch\n",
    "        for sample in self.train_loader:\n",
    "            if self.device_id != 'cpu':\n",
    "                input_ = [data.cuda(self.device_id) for data in sample['input']]\n",
    "                target = [data.cuda(self.device_id) for data in sample['target']]\n",
    "            else:\n",
    "                input_ = [data for data in sample['input']]\n",
    "                target = [data for data in sample['target']]\n",
    "\n",
    "            self.model.G_optimizer.zero_grad()\n",
    "            self.model.D_optimizer.zero_grad()\n",
    "            \n",
    "            # do I need this synch flag?\n",
    "            self.synchronized = False\n",
    "            \n",
    "            output = self.model.G(input_)\n",
    "            loss_g_rec = 0\n",
    "            for scaled_output, scaled_target in zip(output, target):\n",
    "                loss_g_rec += self.model.reconstruction_loss(scaled_output, scaled_target)\n",
    "\n",
    "            # update discriminator\n",
    "            high_res_output = output[-1]\n",
    "            high_res_target = target[-1]\n",
    "            d_fake = self.model.D(high_res_output.detach())\n",
    "            d_real = self.model.D(high_res_target)    \n",
    "            label_fake = torch.zeros_like(d_fake)\n",
    "            label_real = torch.ones_like(d_real)    \n",
    "            loss_d = self.model.adversarial_loss(d_fake, label_fake) + self.model.adversarial_loss(d_real, label_real)\n",
    "            loss_d.backward()\n",
    "            self.model.D_optimizer.step()\n",
    "\n",
    "            # update generator\n",
    "            # do I need a barrier here or not?\n",
    "            if self.distributed:\n",
    "                dist.barrier()\n",
    "            d_fake_with_gradient = self.model.D(high_res_output)\n",
    "            loss_g_adversarial = self.model.adversarial_loss(d_fake_with_gradient, label_real)\n",
    "            loss_g = loss_g_adversarial + loss_g_rec\n",
    "            loss_g.backward()\n",
    "            self.model.G_optimizer.step()\n",
    "            \n",
    "            self.count+=1\n",
    "        \n",
    "        if self.distributed:\n",
    "            dist.barrier()\n",
    "            if not self.synchronized:\n",
    "                self.all_reduce()\n",
    "        self.count = 0\n",
    "        if self.device_id == 0 and epoch % 5 == 0:\n",
    "            self.model.save(\"epoch_{}.pth\".format(epoch, self.device_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01_model.ipynb.\n",
      "Converted 02_architecture_common.ipynb.\n",
      "Converted 03_architecture_MSResNet.ipynb.\n",
      "Converted 04_dataset_common.ipynb.\n",
      "Converted 05_dataset_MSResNet.ipynb.\n",
      "Converted 06_trainer_MSResNet.ipynb.\n",
      "Converted 99_diffaugment.ipynb.\n",
      "Converted Tutorial.ipynb.\n",
      "Converted lightning_trial.ipynb.\n",
      "Converted trials.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:defocus] *",
   "language": "python",
   "name": "conda-env-defocus-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data.common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "from skimage.color import rgb2hsv, hsv2rgb\n",
    "'''\n",
    "AUGMENTATIONS\n",
    "'''            \n",
    "def add_gaussian_noise(img, sigma_sigma=2, rgb_range=255):    \n",
    "    # TODO: note that this is highly unrealistic.\n",
    "    # implement poisson. also a differentiable (pytorch?) version.\n",
    "    # here is a discussion and scikit implementation of poisson\n",
    "    # https://github.com/yu4u/noise2noise/issues/14\n",
    "    '''\n",
    "    image = input_\n",
    "    vals = len(np.unique(image))\n",
    "    vals = 2 ** np.ceil(np.log2(vals))\n",
    "    out = np.random.poisson(image * vals) / float(vals)\n",
    "    out = np.clip(out, 0, 255).astype(np.uint8)\n",
    "    '''\n",
    "    # the default's are from Seungjun again\n",
    "    # https://github.com/SeungjunNah/DeepDeblur-PyTorch/blob/master/src/data/common.py#L43\n",
    "    sigma = np.random.normal() * sigma_sigma * rgb_range/255\n",
    "    # I think casting to 32-bit here is redundant, but let's keep with Seungjun\n",
    "    noise = np.random.randn(*img.shape).astype(np.float32) * sigma \n",
    "    return (img + noise).clip(0, rgb_range)\n",
    "\n",
    "def crop(img, patch_x, patch_y, crop_size=256):        \n",
    "    # be careful about the patch location. it should be:\n",
    "    # patch_y = random.randrange(0, H-patch_size+1)\n",
    "    # patch_x = random.randrange(0, W-patch_size+1)\n",
    "    return img[patch_y:patch_y+crop_size, patch_x:patch_x+crop_size, :]\n",
    "\n",
    "def hflip(img, p=0.5): \n",
    "    return img[:, ::-1, :]\n",
    "\n",
    "def vflip(img): \n",
    "    return img[::-1, :, :]\n",
    "\n",
    "def rot90(img): \n",
    "    return img.transpose(1, 0, 2)\n",
    "\n",
    "def channel_shuffle(img, rgb_order=[0,1,2]): \n",
    "    return img[..., rgb_order]\n",
    "\n",
    "def saturation(img, modifier=1.0, rgb_range=255):\n",
    "    # slooooow.\n",
    "    # TODO: maybe just use g(x)=αf(x)+β, i.e. contrast and brightness?\n",
    "    # if not, maybe play with LAB space, it is slow anyway\n",
    "    # TODO: try using cv2 instead of skimage\n",
    "    # for now, I am keeping Seungjun's version\n",
    "    hsv_img = rgb2hsv(img)\n",
    "    hsv_img[..., 1] *= modifier\n",
    "    return hsv2rgb(hsv_img).clip(0, 1) * rgb_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01_model.ipynb.\n",
      "Converted 02_network_common.ipynb.\n",
      "Converted 03_network_MSResNet.ipynb.\n",
      "Converted 04_dataset_common.ipynb.\n",
      "Converted 05_dataset_MSResNet.ipynb.\n",
      "Converted 99_diffaugment.ipynb.\n",
      "Converted trials.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:defocus] *",
   "language": "python",
   "name": "conda-env-defocus-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

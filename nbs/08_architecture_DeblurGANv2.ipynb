{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp architecture.DeblurGANv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import functools\n",
    "import torch.nn.functional as F\n",
    "\n",
    "'''\n",
    "TODO: as soon as (ever?) torchvision has this, switch to torchvision\n",
    "'''\n",
    "from pretrainedmodels import inceptionresnetv2\n",
    "\n",
    "'''\n",
    "Copied from DeblurGANv2 repo.\n",
    "https://github.com/VITA-Group/DeblurGANv2\n",
    "TODO: simplify the code. seriously.\n",
    "'''\n",
    "instance_norm = functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=True)\n",
    "# Defines the PatchGAN discriminator with the specified arguments.\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_nc=3, ndf=64, n_layers=3, norm_layer=instance_norm, use_sigmoid=False, use_parallel=True):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.use_parallel = use_parallel\n",
    "        if type(norm_layer) == functools.partial:\n",
    "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
    "        else:\n",
    "            use_bias = norm_layer == nn.InstanceNorm2d\n",
    "\n",
    "        kw = 4\n",
    "        padw = int(np.ceil((kw-1)/2))\n",
    "        sequence = [\n",
    "            nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        ]\n",
    "\n",
    "        nf_mult = 1\n",
    "        for n in range(1, n_layers):\n",
    "            nf_mult_prev = nf_mult\n",
    "            nf_mult = min(2**n, 8)\n",
    "            sequence += [\n",
    "                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n",
    "                          kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n",
    "                norm_layer(ndf * nf_mult),\n",
    "                nn.LeakyReLU(0.2, True)\n",
    "            ]\n",
    "\n",
    "        nf_mult_prev = nf_mult\n",
    "        nf_mult = min(2**n_layers, 8)\n",
    "        sequence += [\n",
    "            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n",
    "                      kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n",
    "            norm_layer(ndf * nf_mult),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        ]\n",
    "\n",
    "        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]\n",
    "\n",
    "        if use_sigmoid:\n",
    "            sequence += [nn.Sigmoid()]\n",
    "\n",
    "        self.model = nn.Sequential(*sequence)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.model(input)\n",
    "    \n",
    "    \n",
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, norm_layer=instance_norm, output_ch=3, num_filters=128, num_filters_fpn=256):\n",
    "        super().__init__()\n",
    "\n",
    "        # Feature Pyramid Network (FPN) with four feature maps of resolutions\n",
    "        # 1/4, 1/8, 1/16, 1/32 and `num_filters` filters for all feature maps.\n",
    "        self.fpn = FPN(num_filters=num_filters_fpn, norm_layer=norm_layer)\n",
    "\n",
    "        # The segmentation heads on top of the FPN\n",
    "\n",
    "        self.head1 = FPNHead(num_filters_fpn, num_filters, num_filters)\n",
    "        self.head2 = FPNHead(num_filters_fpn, num_filters, num_filters)\n",
    "        self.head3 = FPNHead(num_filters_fpn, num_filters, num_filters)\n",
    "        self.head4 = FPNHead(num_filters_fpn, num_filters, num_filters)\n",
    "\n",
    "        self.smooth = nn.Sequential(\n",
    "            nn.Conv2d(4 * num_filters, num_filters, kernel_size=3, padding=1),\n",
    "            norm_layer(num_filters),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.smooth2 = nn.Sequential(\n",
    "            nn.Conv2d(num_filters, num_filters // 2, kernel_size=3, padding=1),\n",
    "            norm_layer(num_filters // 2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.final = nn.Conv2d(num_filters // 2, output_ch, kernel_size=3, padding=1)\n",
    "\n",
    "    def unfreeze(self):\n",
    "        self.fpn.unfreeze()\n",
    "\n",
    "    def forward(self, x):\n",
    "        map0, map1, map2, map3, map4 = self.fpn(x)\n",
    "\n",
    "        map4 = nn.functional.upsample(self.head4(map4), scale_factor=8, mode=\"nearest\")\n",
    "        map3 = nn.functional.upsample(self.head3(map3), scale_factor=4, mode=\"nearest\")\n",
    "        map2 = nn.functional.upsample(self.head2(map2), scale_factor=2, mode=\"nearest\")\n",
    "        map1 = nn.functional.upsample(self.head1(map1), scale_factor=1, mode=\"nearest\")\n",
    "\n",
    "        smoothed = self.smooth(torch.cat([map4, map3, map2, map1], dim=1))\n",
    "        smoothed = nn.functional.upsample(smoothed, scale_factor=2, mode=\"nearest\")\n",
    "        smoothed = self.smooth2(smoothed + map0)\n",
    "        smoothed = nn.functional.upsample(smoothed, scale_factor=2, mode=\"nearest\")\n",
    "\n",
    "        final = self.final(smoothed)\n",
    "        res = torch.tanh(final) + x\n",
    "\n",
    "        return torch.clamp(res, min = -1,max = 1)\n",
    "    \n",
    "    \n",
    "class FPNHead(nn.Module):\n",
    "    def __init__(self, num_in, num_mid, num_out):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block0 = nn.Conv2d(num_in, num_mid, kernel_size=3, padding=1, bias=False)\n",
    "        self.block1 = nn.Conv2d(num_mid, num_out, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.block0(x), inplace=True)\n",
    "        x = nn.functional.relu(self.block1(x), inplace=True)\n",
    "        return x    \n",
    "    \n",
    "    \n",
    "class FPN(nn.Module):\n",
    "\n",
    "    def __init__(self, norm_layer=instance_norm, num_filters=256):\n",
    "        \"\"\"Creates an `FPN` instance for feature extraction.\n",
    "        Args:\n",
    "          num_filters: the number of filters in each output pyramid level\n",
    "          pretrained: use ImageNet pre-trained backbone feature extractor\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.inception = inceptionresnetv2(num_classes=1000, pretrained='imagenet')\n",
    "\n",
    "        self.enc0 = self.inception.conv2d_1a\n",
    "        self.enc1 = nn.Sequential(\n",
    "            self.inception.conv2d_2a,\n",
    "            self.inception.conv2d_2b,\n",
    "            self.inception.maxpool_3a,\n",
    "        ) # 64\n",
    "        self.enc2 = nn.Sequential(\n",
    "            self.inception.conv2d_3b,\n",
    "            self.inception.conv2d_4a,\n",
    "            self.inception.maxpool_5a,\n",
    "        )  # 192\n",
    "        self.enc3 = nn.Sequential(\n",
    "            self.inception.mixed_5b,\n",
    "            self.inception.repeat,\n",
    "            self.inception.mixed_6a,\n",
    "        )   # 1088\n",
    "        self.enc4 = nn.Sequential(\n",
    "            self.inception.repeat_1,\n",
    "            self.inception.mixed_7a,\n",
    "        ) #2080\n",
    "        self.td1 = nn.Sequential(nn.Conv2d(num_filters, num_filters, kernel_size=3, padding=1),\n",
    "                                 norm_layer(num_filters),\n",
    "                                 nn.ReLU(inplace=True))\n",
    "        self.td2 = nn.Sequential(nn.Conv2d(num_filters, num_filters, kernel_size=3, padding=1),\n",
    "                                 norm_layer(num_filters),\n",
    "                                 nn.ReLU(inplace=True))\n",
    "        self.td3 = nn.Sequential(nn.Conv2d(num_filters, num_filters, kernel_size=3, padding=1),\n",
    "                                 norm_layer(num_filters),\n",
    "                                 nn.ReLU(inplace=True))\n",
    "        self.pad = nn.ReflectionPad2d(1)\n",
    "        self.lateral4 = nn.Conv2d(2080, num_filters, kernel_size=1, bias=False)\n",
    "        self.lateral3 = nn.Conv2d(1088, num_filters, kernel_size=1, bias=False)\n",
    "        self.lateral2 = nn.Conv2d(192, num_filters, kernel_size=1, bias=False)\n",
    "        self.lateral1 = nn.Conv2d(64, num_filters, kernel_size=1, bias=False)\n",
    "        self.lateral0 = nn.Conv2d(32, num_filters // 2, kernel_size=1, bias=False)\n",
    "\n",
    "        for param in self.inception.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def unfreeze(self):\n",
    "        for param in self.inception.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Bottom-up pathway, from ResNet\n",
    "        enc0 = self.enc0(x)\n",
    "\n",
    "        enc1 = self.enc1(enc0) # 256\n",
    "\n",
    "        enc2 = self.enc2(enc1) # 512\n",
    "\n",
    "        enc3 = self.enc3(enc2) # 1024\n",
    "\n",
    "        enc4 = self.enc4(enc3) # 2048\n",
    "\n",
    "        # Lateral connections\n",
    "\n",
    "        lateral4 = self.pad(self.lateral4(enc4))\n",
    "        lateral3 = self.pad(self.lateral3(enc3))\n",
    "        lateral2 = self.lateral2(enc2)\n",
    "        lateral1 = self.pad(self.lateral1(enc1))\n",
    "        lateral0 = self.lateral0(enc0)\n",
    "\n",
    "        # Top-down pathway\n",
    "        pad = (1, 2, 1, 2)  # pad last dim by 1 on each side\n",
    "        pad1 = (0, 1, 0, 1)\n",
    "        map4 = lateral4\n",
    "        map3 = self.td1(lateral3 + nn.functional.upsample(map4, scale_factor=2, mode=\"nearest\"))\n",
    "        map2 = self.td2(F.pad(lateral2, pad, \"reflect\") + nn.functional.upsample(map3, scale_factor=2, mode=\"nearest\"))\n",
    "        map1 = self.td3(lateral1 + nn.functional.upsample(map2, scale_factor=2, mode=\"nearest\"))\n",
    "        return F.pad(lateral0, pad1, \"reflect\"), map1, map2, map3, map4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01_model.ipynb.\n",
      "Converted 02_architecture_common.ipynb.\n",
      "Converted 03_architecture_MSResNet.ipynb.\n",
      "Converted 04_dataset_common.ipynb.\n",
      "Converted 05_dataset_MSResNet.ipynb.\n",
      "Converted 06_trainer_MSResNet.ipynb.\n",
      "Converted 07_metrics.ipynb.\n",
      "Converted 08_architecture_DeblurGANv2.ipynb.\n",
      "Converted 09_dataset_DeblurGANv2.ipynb.\n",
      "Converted 99_diffaugment.ipynb.\n",
      "Converted Tutorial_without_lightning.ipynb.\n",
      "Converted model_without_lightning.ipynb.\n",
      "Converted trials.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_disc = Discriminator(n_layers=3,\n",
    "                           norm_layer=functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=True),\n",
    "                           use_sigmoid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(norm_layer=functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
